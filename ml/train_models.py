import argparse
from pathlib import Path
from typing import Optional, Tuple, Dict, Any

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    f1_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    silhouette_score,
)
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC


OUTPUT_DIR = Path("ml_output")
MODELS_DIR = OUTPUT_DIR / "models"
PLOTS_DIR = OUTPUT_DIR / "plots"


def ensure_dirs() -> None:
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    PLOTS_DIR.mkdir(parents=True, exist_ok=True)


def load_csv_dataset(path: Path, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ CSV.

    path: –ø—É—Ç—å –∫ csv
    target_col: –∏–º—è —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
    """

    df = pd.read_csv(path)
    if target_col not in df.columns:
        raise ValueError(f"–°—Ç–æ–ª–±–µ—Ü '{target_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ CSV {path}")
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥: –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    X = X.select_dtypes(include=[np.number]).copy()
    return X, y


def remove_outliers(
    X: pd.DataFrame, 
    y: pd.Series, 
    threshold: float = 3.0, 
    max_value: Optional[float] = 1000.0
) -> Tuple[pd.DataFrame, pd.Series]:
    """–£–¥–∞–ª—è–µ—Ç –≤—ã–±—Ä–æ—Å—ã –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –∞–±—Å–æ–ª—é—Ç–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä + z-score.
    
    Args:
        X: –ø—Ä–∏–∑–Ω–∞–∫–∏
        y: —Ç–∞—Ä–≥–µ—Ç
        threshold: –ø–æ—Ä–æ–≥ z-score (–æ–±—ã—á–Ω–æ 3.0)
        max_value: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ; –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—à–µ —É–¥–∞–ª—è—é—Ç—Å—è –ø–µ—Ä–≤—ã–º–∏
                   (None —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å)
    
    Returns:
        X –∏ y –±–µ–∑ –≤—ã–±—Ä–æ—Å–æ–≤
    """
    from scipy import stats
    
    original_len = len(y)
    
    # –≠—Ç–∞–ø 1: –∞–±—Å–æ–ª—é—Ç–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é
    if max_value is not None:
        mask_abs = y <= max_value
        removed_abs = (~mask_abs).sum()
        if removed_abs > 0:
            print(f"‚ö†Ô∏è  –≠—Ç–∞–ø 1 - –£–¥–∞–ª–µ–Ω–æ {removed_abs} –∑–Ω–∞—á–µ–Ω–∏–π –≤—ã—à–µ {max_value}s")
            print(f"   –î–∏–∞–ø–∞–∑–æ–Ω —É–¥–∞–ª—ë–Ω–Ω—ã—Ö: {y[~mask_abs].min():.2f} - {y[~mask_abs].max():.2f}s")
        X = X[mask_abs]
        y = y[mask_abs]
    else:
        removed_abs = 0
    
    # –≠—Ç–∞–ø 2: z-score —Ñ–∏–ª—å—Ç—Ä –Ω–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
    if len(y) > 0:
        z_scores = np.abs(stats.zscore(y))
        mask_zscore = z_scores < threshold
        removed_zscore = (~mask_zscore).sum()
        if removed_zscore > 0:
            print(f"‚ö†Ô∏è  –≠—Ç–∞–ø 2 - –£–¥–∞–ª–µ–Ω–æ {removed_zscore} –≤—ã–±—Ä–æ—Å–æ–≤ (z-score > {threshold})")
            print(f"   –î–∏–∞–ø–∞–∑–æ–Ω —É–¥–∞–ª—ë–Ω–Ω—ã—Ö: {y[~mask_zscore].min():.2f} - {y[~mask_zscore].max():.2f}s")
        X = X[mask_zscore]
        y = y[mask_zscore]
    
    total_removed = original_len - len(y)
    print(f"\nüìä –ò—Ç–æ–≥–æ: —É–¥–∞–ª–µ–Ω–æ {total_removed}/{original_len} —Å—ç–º–ø–ª–æ–≤ ({100*total_removed/original_len:.1f}%)")
    print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {len(y)} —Å—ç–º–ø–ª–æ–≤\n")
    
    return X, y


def prepare_classification_dataset(
    X: pd.DataFrame,
    y: pd.Series,
    *,
    max_bins: int = 6,
    min_per_class: int = 2,
) -> Tuple[Optional[pd.DataFrame], Optional[pd.Series], pd.Series]:
    """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∫ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º –∫–ª–∞—Å—Å–∞–º –∏ —É–±–∏—Ä–∞–µ—Ç —Ä–µ–¥–∫–∏–µ –º–µ—Ç–∫–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (X_filtered, y_filtered, class_counts). –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤
    –º–µ–Ω—å—à–µ –¥–≤—É—Ö –∏–ª–∏ –≤—ã–±–æ—Ä–∫–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (None, None, class_counts).
    """

    y_proc = y.copy()

    # –ï—Å–ª–∏ target —á–∏—Å–ª–æ–≤–æ–π –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–Ω–æ–≥–æ, –±–∏–Ω–Ω–∏–º –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—è–º,
    # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–ª–∞—Å—Å–æ–≤ —Å –æ–¥–Ω–∏–º –æ–±—ä–µ–∫—Ç–æ–º.
    if np.issubdtype(y_proc.dtype, np.number) and y_proc.nunique() > max_bins:
        y_proc = pd.qcut(y_proc, q=max_bins, duplicates="drop")

    y_proc = y_proc.astype(str)

    # –£–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å—ã —Ä–µ–∂–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
    value_counts = y_proc.value_counts()
    rare_classes = value_counts[value_counts < min_per_class].index
    if len(rare_classes):
        mask = ~y_proc.isin(rare_classes)
        y_proc = y_proc[mask]
        X = X.loc[mask]

    value_counts = y_proc.value_counts()

    if y_proc.nunique() < 2 or len(y_proc) < 4:
        return None, None, value_counts

    return X, y_proc, value_counts


def run_regression(X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
    print("=== –†–µ–≥—Ä–µ—Å—Å–∏—è –≤—Ä–µ–º–µ–Ω–∏ OCR ===")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞—Ä–≥–µ—Ç–∞
    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ OCR (—Å–µ–∫—É–Ω–¥—ã):")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {y.mean():.2f}")
    print(f"  –ú–µ–¥–∏–∞–Ω–∞: {y.median():.2f}")
    print(f"  –ú–∏–Ω: {y.min():.2f}, –ú–∞–∫—Å: {y.max():.2f}")
    print(f"  Std: {y.std():.2f}")
    
    # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞:
    # 1. –ê–±—Å–æ–ª—é—Ç–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä: –≤—Å–µ –≤—ã—à–µ 1000 —Å–µ–∫—É–Ω–¥ —É–¥–∞–ª—è—é—Ç—Å—è
    # 2. Z-score —Ñ–∏–ª—å—Ç—Ä: —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–±—Ä–æ—Å—ã –≤ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
    print(f"\n{'='*50}")
    print(f"–û—á–∏—Å—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (2 —ç—Ç–∞–ø–∞)...")
    print(f"{'='*50}")
    X_clean, y_clean = remove_outliers(X, y, threshold=3.0, max_value=1000.0)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    X_enhanced = X_clean.copy()
    if 'megapixels' in X_clean.columns:
        X_enhanced['megapixels_squared'] = X_clean['megapixels'] ** 2
        X_enhanced['log_megapixels'] = np.log1p(X_clean['megapixels'])
    if 'width' in X_clean.columns and 'height' in X_clean.columns:
        X_enhanced['aspect_ratio'] = X_clean['width'] / (X_clean['height'] + 1)
        X_enhanced['total_pixels'] = X_clean['width'] * X_clean['height']
        X_enhanced['log_pixels'] = np.log1p(X_enhanced['total_pixels'])
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_enhanced, y_clean, test_size=0.2, random_state=42
    )

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Random Forest –≤–º–µ—Å—Ç–æ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    print(f"\n–û–±—É—á–µ–Ω–∏–µ Random Forest...")
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = pd.DataFrame({
        'feature': X_enhanced.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(f"\n–¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for _, row in feature_importance.head(5).iterrows():
        print(f"  {row['feature']}: {row['importance']:.3f}")

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)

    print(f"R^2: {r2:.3f}")
    print(f"MAE: {mae:.3f}")
    print(f"MSE: {mse:.3f}")

    # –ì—Ä–∞—Ñ–∏–∫–∏: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ vs —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    plt.figure(figsize=(6, 5))
    plt.scatter(y_test, y_pred, alpha=0.6)
    plt.xlabel("–§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ")
    plt.ylabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ")
    plt.title("–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è: y_true vs y_pred")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "regression_scatter.png")
    plt.close()

    errors = y_test - y_pred
    plt.figure(figsize=(6, 5))
    plt.hist(errors, bins=30, alpha=0.7)
    plt.xlabel("–û—à–∏–±–∫–∞ (y_true - y_pred)")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "regression_errors_hist.png")
    plt.close()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤–º–µ—Å—Ç–µ —Å –ø–æ—Ä—è–¥–∫–æ–º —Ñ–∏—á–µ–π –∏ scaler
    payload = {
        "model": model,
        "scaler": scaler,
        "feature_names": list(X_enhanced.columns),
    }
    joblib.dump(payload, MODELS_DIR / "ocr_time_regression.joblib")

    return {"r2": float(r2), "mae": float(mae), "mse": float(mse)}


def run_classification(X: pd.DataFrame, y: pd.Series) -> Dict[str, Dict[str, float]]:
    print("=== –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, SVM, MLP ===")
    n_classes = len(np.unique(y))
    n_samples = len(y)
    test_size = max(0.2, n_classes / n_samples + 0.01)
    test_size = min(test_size, 0.5)  # –Ω–µ –æ—Ç–¥–∞—ë–º –≤ —Ç–µ—Å—Ç –±–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    results = {}

    # –î–ª—è F1 —Ä–∞–∑–ª–∏—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–π –∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π —Å–ª—É—á–∞–∏
    average_type = "binary" if len(np.unique(y_train)) == 2 else "weighted"

    # –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
    log_reg = LogisticRegression(max_iter=500)
    log_reg.fit(X_train_scaled, y_train)
    y_pred_lr = log_reg.predict(X_test_scaled)
    results["LogReg"] = {
        "acc": accuracy_score(y_test, y_pred_lr),
        "f1": f1_score(y_test, y_pred_lr, average=average_type),
    }

    # SVM (RBF)
    svm = SVC(kernel="rbf", probability=True)
    svm.fit(X_train_scaled, y_train)
    y_pred_svm = svm.predict(X_test_scaled)
    results["SVM"] = {
        "acc": accuracy_score(y_test, y_pred_svm),
        "f1": f1_score(y_test, y_pred_svm, average=average_type),
    }

    # MLP (–Ω–µ–π—Ä–æ—Å–µ—Ç—å)
    mlp = MLPClassifier(hidden_layer_sizes=(32, 16), max_iter=1000, random_state=42)
    mlp.fit(X_train_scaled, y_train)
    y_pred_mlp = mlp.predict(X_test_scaled)
    results["MLP"] = {
        "acc": accuracy_score(y_test, y_pred_mlp),
        "f1": f1_score(y_test, y_pred_mlp, average=average_type),
    }

    for name, metrics in results.items():
        print(f"\n{name}:")
        print(f"  Accuracy: {metrics['acc']:.3f}")
        print(f"  F1-score: {metrics['f1']:.3f}")

    print("\n–û—Ç—á—ë—Ç –ø–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ (–ø–æ F1):")
    best_name = max(results, key=lambda k: results[k]["f1"])
    if best_name == "LogReg":
        best_pred = y_pred_lr
    elif best_name == "SVM":
        best_pred = y_pred_svm
    else:
        best_pred = y_pred_mlp

    print(classification_report(y_test, best_pred))

    # –ì—Ä–∞—Ñ–∏–∫: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ accuracy –∏ F1
    names = list(results.keys())
    accs = [results[n]["acc"] for n in names]
    f1s = [results[n]["f1"] for n in names]

    x = np.arange(len(names))
    width = 0.35
    plt.figure(figsize=(6, 5))
    plt.bar(x - width / 2, accs, width, label="Accuracy")
    plt.bar(x + width / 2, f1s, width, label="F1-score")
    plt.xticks(x, names)
    plt.ylim(0, 1.0)
    plt.legend()
    plt.title("–ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "classification_models.png")
    plt.close()

    joblib.dump(mlp, MODELS_DIR / "mlp_classifier.joblib")

    return {name: {"acc": float(m["acc"]), "f1": float(m["f1"])} for name, m in results.items()}


def run_clustering(X: pd.DataFrame, y_true: Optional[pd.Series] = None) -> Dict[str, Any]:
    print("=== –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: k-means ===")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏, –±–µ—Ä—ë–º —á–∏—Å–ª–æ –∏—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    if y_true is not None:
        unique_classes = np.unique(y_true)
        k = max(2, len(unique_classes))
    else:
        k = 5
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)

    sil = silhouette_score(X_scaled, labels)
    print(f"Silhouette score: {sil:.3f}")

    ari = None
    if y_true is not None and len(np.unique(y_true)) == k:
        from sklearn.metrics import adjusted_rand_score

        ari = adjusted_rand_score(y_true, labels)
        print(f"Adjusted Rand Index (–∫–ª–∞—Å—Ç–µ—Ä—ã vs –∏—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã): {ari:.3f}")

    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –≥–ª–∞–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ–≥–æ scatter
    from sklearn.decomposition import PCA

    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    plt.figure(figsize=(6, 5))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap="tab10", alpha=0.7)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title("k-means –∫–ª–∞—Å—Ç–µ—Ä—ã (PCA 2D)")
    plt.tight_layout()
    plt.savefig(PLOTS_DIR / "clustering_kmeans.png")
    plt.close()

    joblib.dump(kmeans, MODELS_DIR / "kmeans.joblib")

    return {"k": int(k), "silhouette": float(sil), "ari": float(ari) if ari is not None else None}


def run_all(csv_path: Optional[str] = None, target_col: Optional[str] = None) -> Dict[str, Any]:
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π ML-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∏ –≤–µ—Ä–Ω—É—Ç—å –º–µ—Ç—Ä–∏–∫–∏.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –∏–∑ CLI (main), —Ç–∞–∫ –∏ –∏–∑ Telegram-–±–æ—Ç–∞.
    –û–∂–∏–¥–∞–µ—Ç—Å—è —Ç–∞–±–ª–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏ –±–æ—Ç–∞), –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –≤ –≤–∏–¥–µ CSV.
    """

    ensure_dirs()

    if not csv_path or not target_col:
        raise ValueError(
            "–ù—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å --csv –∏ --target-col. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CSV —Å –ª–æ–≥–∞–º–∏ –±–æ—Ç–∞ –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ç–∞–±–ª–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç."
        )

    csv_path = Path(csv_path)
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")

    print(f"–ó–∞–≥—Ä—É–∂–∞—é CSV –¥–∞—Ç–∞—Å–µ—Ç: {csv_path}")
    X_full, y = load_csv_dataset(csv_path, target_col)

    if not np.issubdtype(y.dtype, np.number):
        raise ValueError(
            "–î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω—É–∂–µ–Ω —á–∏—Å–ª–æ–≤–æ–π —á–∏—Å–ª–æ–≤–æ–π target; –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ CSV, –≥–¥–µ —Ü–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ ‚Äî —á–∏—Å–ª–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)."
        )
    y_reg = y.astype(float)

    # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å—Ç–∞—Ä–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ –∂–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —á—Ç–æ –∏ –≤ –±–æ—Ç–µ
    preferred_cols = [
        "width",
        "height",
        "megapixels",
        "brightness",
        "contrast",
        "word_count",
    ]
    available = [c for c in preferred_cols if c in X_full.columns]
    if available:
        X_reg = X_full[available].copy()
    else:
        X_reg = X_full

    print(f"–§–æ—Ä–º–∞ X_reg (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏): {X_reg.shape}")
    print(f"–§–æ—Ä–º–∞ X_full (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏/–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏): {X_full.shape}")

    metrics_reg = run_regression(X_reg, y_reg)

    X_cls, y_cls, class_counts = prepare_classification_dataset(X_full, y_reg)
    if X_cls is None or y_cls is None:
        print(
            "–ü—Ä–æ–ø—É—Å–∫–∞—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é/–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é: –ø–æ—Å–ª–µ –±–∏–Ω–Ω–∏–Ω–≥–∞/—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–µ–Ω—å—à–µ –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤."
        )
        if not class_counts.empty:
            print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
            print(class_counts.to_string())
        metrics_cls = {}
        metrics_clu = {}
    else:
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –±–∏–Ω–Ω–∏–Ω–≥–∞/—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
        print(class_counts.to_string())
        metrics_cls = run_classification(X_cls, y_cls)
        metrics_clu = run_clustering(X_cls, y_cls)

    return {
        "n_samples": int(X_full.shape[0]),
        "n_features": int(X_full.shape[1]),
        "regression": metrics_reg,
        "classification": metrics_cls,
        "clustering": metrics_clu,
    }


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "–£—á–µ–±–Ω—ã–π ML-–ø—Ä–æ–µ–∫—Ç: —Ä–µ–≥—Ä–µ—Å—Å–∏—è, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å "
            "–Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∞—à CSV (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–≥–∏ –±–æ—Ç–∞)."
        )
    )
    parser.add_argument(
        "--csv",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ —Å–≤–æ–µ–º—É CSV (–Ω–∞–ø—Ä–∏–º–µ—Ä, ml_output/events.csv —Å –ª–æ–≥–∞–º–∏ –±–æ—Ç–∞)",
    )
    parser.add_argument(
        "--target-col",
        type=str,
        default=None,
        help=(
            "–ò–º—è —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è CSV. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–≥–∏ –±–æ—Ç–∞ –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ç–∞–±–ª–∏—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç."
        ),
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    _ = run_all(args.csv, args.target_col)
    print("\n–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ:", OUTPUT_DIR.resolve())


if __name__ == "__main__":
    main()
